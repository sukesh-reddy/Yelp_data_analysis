{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session object\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"yelp_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from amazon s3 bucket\n",
    "yelp_review = spark.read.csv('s3://yelpdatanalysis/yelp_reviews.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               stars|                text|\n",
      "+--------------------+--------------------+\n",
      "|                   1|Total bill for th...|\n",
      "|                   5|I *adore* Travis ...|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                   5|I have to say tha...|\n",
      "|                   5|Went in for a lun...|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                   1|\"Today was my sec...|\n",
      "|                   4|\"I'll be the firs...|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                null|                null|\n",
      "|                   3|Tracy dessert had...|\n",
      "|                null|                null|\n",
      "| more than $5 is ...| it is Tracy Dessert|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and Prepare the Data\n",
    "\n",
    "df_reviews = yelp_review.select('stars','text')\n",
    "\n",
    "filterd_df_1 = df_reviews.na.drop(subset=['stars','text'])\n",
    "\n",
    "filter_df_2 = filterd_df_1.select(filterd_df_1.stars.cast('float'),filterd_df_1.text.cast('string')).na.drop(subset=['stars','text'])\n",
    "filter_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stop words and cleaning reviews\n",
    "import string\n",
    "import re\n",
    "\n",
    "def remove_punct(text):\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  \n",
    "    return nopunct\n",
    "\n",
    "# giving a class label to the rating\n",
    "\n",
    "def convert_rating(rating):\n",
    "    if rating >=4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|stars|\n",
      "+--------------------+-----+\n",
      "|Total bill for th...|    0|\n",
      "|I  adore  Travis ...|    1|\n",
      "|I have to say tha...|    1|\n",
      "|Went in for a lun...|    1|\n",
      "| Today was my sec...|    0|\n",
      "| I ll be the firs...|    1|\n",
      "|Tracy dessert had...|    0|\n",
      "|This place has go...|    0|\n",
      "| I was really loo...|    0|\n",
      "|It s a giant Best...|    0|\n",
      "|Like walking back...|    1|\n",
      "|Walked in around ...|    0|\n",
      "|Wow  So surprised...|    1|\n",
      "|Michael from Red ...|    1|\n",
      "|I cannot believe ...|    0|\n",
      "|You can t really ...|    1|\n",
      "|Great lunch today...|    1|\n",
      "| I love chinese f...|    0|\n",
      "|We ve been a huge...|    1|\n",
      "|Good selection of...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "punct_remover = udf(lambda x: remove_punct(x))\n",
    "rating_convert = udf(lambda x: convert_rating(x))\n",
    "\n",
    "resultDF = filter_df_2.select(punct_remover('text'), rating_convert('stars'))\n",
    "\n",
    "#user defined functions change column names so we rename the columns back to its original names\n",
    "resultDF = resultDF.withColumnRenamed('<lambda>(text)', 'text')\n",
    "resultDF = resultDF.withColumnRenamed('<lambda>(stars)', 'stars')\n",
    "\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|stars|       avg(length)|\n",
      "+-----+------------------+\n",
      "|    0| 358.4106661671254|\n",
      "|    1|295.06875408579606|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ** Create a new length feature: **\n",
    "from pyspark.sql.functions import length\n",
    "resultDF.withColumn('length',length(resultDF['text'])).groupBy('stars').mean().show()\n",
    "resultDF = resultDF.withColumnRenamed('stars','label')\n",
    "\n",
    "# There isn't much Difference, hence it cannot be is used as attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Transformations\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# We will be using simple naive bayes model\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "data_prep_pipe = Pipeline(stages=[tokenizer,stopremove,count_vec,idf,clean_up])\n",
    "\n",
    "cleaner = data_prep_pipe.fit(resultDF)\n",
    "\n",
    "clean_data = cleaner.transform(resultDF)\n",
    "\n",
    "clean_data = clean_data.select(['label','features'])\n",
    "\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "\n",
    "(training,testing) = clean_data.randomSplit([0.7,0.3])\n",
    "\n",
    "training.printSchema()\n",
    "\n",
    "training = training.select('features',training.label.cast('integer'))\n",
    "testing = testing.select('features',testing.label.cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|       (80152,[],[])|    0|[-1.0715920695799...|[0.34246285847122...|       1.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-4374.7909341232...|[0.99999999999999...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-4274.0765000638...|[4.80832995934765...|       1.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-1797.2194013785...|[0.99999999999999...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-1640.9987822162...|[1.0,5.0376493933...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-2288.2264636044...|[1.87425164908585...|       1.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-761.51374012714...|[1.0,7.6121040044...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-687.69770395469...|[0.00302243577364...|       1.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-5496.4596506964...|[1.0,7.5411797821...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-4777.7123135063...|[1.0,1.9760282399...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-3421.8557253718...|[1.0,6.4396746572...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-1004.2871700417...|[8.75269163058108...|       1.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-974.20831657548...|[1.0,1.5439822368...|       0.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-668.63236377027...|[3.66426177733482...|       1.0|\n",
      "|(80152,[0,1,2,3,4...|    0|[-1288.7313284262...|[3.21781170498065...|       1.0|\n",
      "|(80152,[0,1,2,3,5...|    0|[-6711.2476183546...|[0.99999999999999...|       0.0|\n",
      "|(80152,[0,1,2,3,5...|    0|[-1799.4846201290...|[1.0,3.6484055498...|       0.0|\n",
      "|(80152,[0,1,2,3,5...|    0|[-989.11279266391...|[0.99991715539163...|       0.0|\n",
      "|(80152,[0,1,2,3,5...|    0|[-132.81352052496...|[0.35516112465872...|       1.0|\n",
      "|(80152,[0,1,2,3,5...|    0|[-1034.1619907085...|[0.99999998221264...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "spam_predictor = nb.fit(training)\n",
    "\n",
    "test_results = spam_predictor.transform(testing)\n",
    "\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.8169506197443153\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting positive or negative  was: {}\".format(acc))\n",
    "#Not bad considering we're using straight math on text data! \n",
    "# We can Try switching out with multiple classification models! \n",
    "# Or even try to come up with other engineered features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
