{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574753872462_314613044","id":"20191126-073752_1273699710","dateCreated":"2019-11-26T07:37:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3446","text":"%pyspark\r\n# Creating Spark Session object\r\nfrom pyspark.sql import SparkSession\r\nspark = SparkSession.builder.appName(\"yelp_analysis\").getOrCreate()","dateUpdated":"2019-11-26T08:08:44+0000","dateFinished":"2019-11-26T08:08:44+0000","dateStarted":"2019-11-26T08:08:44+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755485654_932113977","id":"20191126-080445_1148147322","dateCreated":"2019-11-26T08:04:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3529","text":"%pyspark\r\n# Loading the data from amazon s3 bucket\r\nyelp_review = spark.read.csv('s3://yelpdataanalysis/yelp_reviews.csv',header=True,inferSchema=True)","dateUpdated":"2019-11-26T08:08:44+0000","dateFinished":"2019-11-26T08:08:55+0000","dateStarted":"2019-11-26T08:08:44+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755486920_-1288114021","id":"20191126-080446_1773301387","dateCreated":"2019-11-26T08:04:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3604","text":"%pyspark\r\n# Clean and Prepare the Data\r\n\r\ndf_reviews = yelp_review.select('stars','text')\r\n\r\nfilterd_df_1 = df_reviews.na.drop(subset=['stars','text'])\r\n\r\nfilter_df_2 = filterd_df_1.select(filterd_df_1.stars.cast('float'),filterd_df_1.text.cast('string')).na.drop(subset=['stars','text'])\r\nfilter_df_2.show()","dateUpdated":"2019-11-26T08:08:55+0000","dateFinished":"2019-11-26T08:08:55+0000","dateStarted":"2019-11-26T08:08:55+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------------------+\n|stars|                text|\n+-----+--------------------+\n|  1.0|Total bill for th...|\n|  5.0|I *adore* Travis ...|\n|  5.0|I have to say tha...|\n|  5.0|Went in for a lun...|\n|  1.0|\"Today was my sec...|\n|  4.0|\"I'll be the firs...|\n|  3.0|Tracy dessert had...|\n|  1.0|This place has go...|\n|  2.0|\"I was really loo...|\n|  3.0|It's a giant Best...|\n|  4.0|Like walking back...|\n|  1.0|Walked in around ...|\n|  4.0|Wow. So surprised...|\n|  4.0|Michael from Red ...|\n|  1.0|I cannot believe ...|\n|  5.0|You can't really ...|\n|  4.0|Great lunch today...|\n|  3.0|\"I love chinese f...|\n|  5.0|We've been a huge...|\n|  3.0|Good selection of...|\n+-----+--------------------+\nonly showing top 20 rows\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755488036_-1497217665","id":"20191126-080448_831406614","dateCreated":"2019-11-26T08:04:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3676","text":"%pyspark\r\n# Removing the stop words and cleaning reviews\r\nimport string\r\nimport re\r\n\r\ndef remove_punct(text):\r\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\r\n    nopunct = regex.sub(\" \", text)  \r\n    return nopunct\r\n\r\n# giving a class label to the rating\r\n\r\ndef convert_rating(rating):\r\n    if rating >=4:\r\n        return 1\r\n    else:\r\n        return 0","dateUpdated":"2019-11-26T08:08:56+0000","dateFinished":"2019-11-26T08:08:56+0000","dateStarted":"2019-11-26T08:08:56+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755490966_2070771027","id":"20191126-080450_1382407362","dateCreated":"2019-11-26T08:04:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3748","text":"%pyspark\r\nfrom pyspark.sql.functions import udf\r\npunct_remover = udf(lambda x: remove_punct(x))\r\nrating_convert = udf(lambda x: convert_rating(x))\r\n\r\nresultDF = filter_df_2.select(punct_remover('text'), rating_convert('stars'))\r\n\r\n#user defined functions change column names so we rename the columns back to its original names\r\nresultDF = resultDF.withColumnRenamed('<lambda>(text)', 'text')\r\nresultDF = resultDF.withColumnRenamed('<lambda>(stars)', 'stars')\r\n\r\nresultDF.show()","dateUpdated":"2019-11-26T08:08:56+0000","dateFinished":"2019-11-26T08:08:58+0000","dateStarted":"2019-11-26T08:08:56+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+\n|                text|stars|\n+--------------------+-----+\n|Total bill for th...|    0|\n|I  adore  Travis ...|    1|\n|I have to say tha...|    1|\n|Went in for a lun...|    1|\n| Today was my sec...|    0|\n| I ll be the firs...|    1|\n|Tracy dessert had...|    0|\n|This place has go...|    0|\n| I was really loo...|    0|\n|It s a giant Best...|    0|\n|Like walking back...|    1|\n|Walked in around ...|    0|\n|Wow  So surprised...|    1|\n|Michael from Red ...|    1|\n|I cannot believe ...|    0|\n|You can t really ...|    1|\n|Great lunch today...|    1|\n| I love chinese f...|    0|\n|We ve been a huge...|    1|\n|Good selection of...|    0|\n+--------------------+-----+\nonly showing top 20 rows\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755492024_564499218","id":"20191126-080452_1139151816","dateCreated":"2019-11-26T08:04:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3820","text":"%pyspark\r\n# ** Create a new length feature: **\r\nfrom pyspark.sql.functions import length\r\nresultDF.withColumn('length',length(resultDF['text'])).groupBy('stars').mean().show()\r\nresultDF = resultDF.withColumnRenamed('stars','label')\r\n\r\n# There isn't much Difference, hence it cannot be is used as attribute\r\n","dateUpdated":"2019-11-26T08:08:58+0000","dateFinished":"2019-11-26T08:09:04+0000","dateStarted":"2019-11-26T08:08:58+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+------------------+\n|stars|       avg(length)|\n+-----+------------------+\n|    0| 358.4106661671254|\n|    1|295.06875408579606|\n+-----+------------------+\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755493352_-1857945211","id":"20191126-080453_1676827919","dateCreated":"2019-11-26T08:04:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3892","text":"%pyspark\r\n# Feature Transformations\r\n\r\nfrom pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF\r\n\r\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\r\nstopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\r\ncount_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\r\nidf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\r\n","dateUpdated":"2019-11-26T08:09:04+0000","dateFinished":"2019-11-26T08:09:04+0000","dateStarted":"2019-11-26T08:09:04+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755494986_-827062772","id":"20191126-080454_355291766","dateCreated":"2019-11-26T08:04:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3964","text":"%pyspark\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.linalg import Vector\r\n\r\nclean_up = VectorAssembler(inputCols=['tf_idf'],outputCol='features')","dateUpdated":"2019-11-26T08:09:04+0000","dateFinished":"2019-11-26T08:09:05+0000","dateStarted":"2019-11-26T08:09:04+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755499170_1650550029","id":"20191126-080459_1978947151","dateCreated":"2019-11-26T08:04:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4036","text":"%pyspark\r\nfrom pyspark.ml.classification import NaiveBayes\r\n# We will be using simple naive bayes model\r\nnb = NaiveBayes()","dateUpdated":"2019-11-26T08:09:05+0000","dateFinished":"2019-11-26T08:09:05+0000","dateStarted":"2019-11-26T08:09:05+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755503932_575438310","id":"20191126-080503_163731607","dateCreated":"2019-11-26T08:05:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4108","text":"%pyspark\r\nfrom pyspark.ml import Pipeline\r\n\r\ndata_prep_pipe = Pipeline(stages=[tokenizer,stopremove,count_vec,idf,clean_up])\r\n\r\ncleaner = data_prep_pipe.fit(resultDF)\r\n\r\nclean_data = cleaner.transform(resultDF)\r\n\r\nclean_data = clean_data.select(['label','features'])\r\n\r\nclean_data.show()","dateUpdated":"2019-11-26T08:09:05+0000","dateFinished":"2019-11-26T08:09:30+0000","dateStarted":"2019-11-26T08:09:05+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|    0|(80152,[0,5,17,22...|\n|    1|(80152,[0,3,5,18,...|\n|    1|(80152,[0,3,8,12,...|\n|    1|(80152,[0,25,29,3...|\n|    0|(80152,[0,5,6,8,1...|\n|    1|(80152,[0,1,4,11,...|\n|    0|(80152,[2,9,24,90...|\n|    0|(80152,[0,1,2,10,...|\n|    0|(80152,[0,4,12,10...|\n|    0|(80152,[0,2,8,14,...|\n|    1|(80152,[0,6,7,10,...|\n|    0|(80152,[0,4,5,14,...|\n|    1|(80152,[0,9,15,20...|\n|    1|(80152,[0,3,4,6,1...|\n|    0|(80152,[0,5,7,8,1...|\n|    1|(80152,[0,1,2,12,...|\n|    1|(80152,[0,1,3,4,1...|\n|    0|(80152,[0,1,2,11,...|\n|    1|(80152,[0,2,3,9,1...|\n|    0|(80152,[0,4,13,15...|\n+-----+--------------------+\nonly showing top 20 rows\n\n"}]}},{"text":"%pyspark\n#Splitting the data\n\n(training,testing) = clean_data.randomSplit([0.7,0.3])\n\ntraining.printSchema()\n\ntraining = training.select('features',training.label.cast('integer'))\ntesting = testing.select('features',testing.label.cast('integer'))","user":"anonymous","dateUpdated":"2019-11-26T08:09:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755555653_1554935607","id":"20191126-080555_597852449","dateCreated":"2019-11-26T08:05:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4197","dateFinished":"2019-11-26T08:09:31+0000","dateStarted":"2019-11-26T08:09:30+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- label: string (nullable = true)\n |-- features: vector (nullable = true)\n\n"}]}},{"text":"%pyspark\n# training the model\nspam_predictor = nb.fit(training)\n\ntest_results = spam_predictor.transform(testing)\n\ntest_results.show()","user":"anonymous","dateUpdated":"2019-11-26T08:09:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755557019_-2100906020","id":"20191126-080557_507128832","dateCreated":"2019-11-26T08:05:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4269","dateFinished":"2019-11-26T08:09:54+0000","dateStarted":"2019-11-26T08:09:31+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+--------------------+--------------------+----------+\n|            features|label|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------------+--------------------+----------+\n|       (80152,[],[])|    0|[-1.0737616963300...|[0.34172064734429...|       1.0|\n|       (80152,[],[])|    0|[-1.0737616963300...|[0.34172064734429...|       1.0|\n|       (80152,[],[])|    0|[-1.0737616963300...|[0.34172064734429...|       1.0|\n|       (80152,[],[])|    0|[-1.0737616963300...|[0.34172064734429...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-4404.0946723859...|[3.14169429702578...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-1296.3175503156...|[0.99998646311423...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-2951.8929448934...|[1.04825889292815...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-3307.5982064010...|[2.31142993988170...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-6069.3641568092...|[1.0,3.5550207656...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-2513.7912741506...|[1.0,1.6502024249...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-759.44892120455...|[0.99527012443285...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-570.25508736558...|[0.00497429300961...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-1867.8393489429...|[1.0,1.2670345678...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-1599.4319706536...|[1.0,1.0931130918...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-1084.9208174000...|[0.99999997732314...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-10509.709578642...|[1.0,5.0852444853...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-1234.7761131512...|[0.29405297137877...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-1409.2030390194...|[1.0,4.2545962171...|       0.0|\n|(80152,[0,1,2,3,4...|    0|[-3738.8673968600...|[6.09720902786404...|       1.0|\n|(80152,[0,1,2,3,4...|    0|[-1475.3473854037...|[0.99999998732437...|       0.0|\n+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"}]}},{"text":"%pyspark\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","user":"anonymous","dateUpdated":"2019-11-26T08:09:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755557767_-1102327468","id":"20191126-080557_277987410","dateCreated":"2019-11-26T08:05:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4341","dateFinished":"2019-11-26T08:09:55+0000","dateStarted":"2019-11-26T08:09:55+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%pyspark\nacc_eval = MulticlassClassificationEvaluator()\nacc = acc_eval.evaluate(test_results)\nprint(\"Accuracy of model at predicting positive or negative  was: {}\".format(acc))\n#Not bad considering we're using straight math on text data! \n# We can Try switching out with multiple classification models! \n# Or even try to come up with other engineered features!","user":"anonymous","dateUpdated":"2019-11-26T08:09:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574755678708_139208790","id":"20191126-080758_576013918","dateCreated":"2019-11-26T08:07:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4413","dateFinished":"2019-11-26T08:10:26+0000","dateStarted":"2019-11-26T08:09:55+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Accuracy of model at predicting positive or negative  was: 0.817770737566\n"}]}}],"name":"yelp_sentiment_analysis","id":"2ETKEW9U4","noteParams":{},"noteForms":{},"angularObjects":{"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}